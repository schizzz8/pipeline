\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}

%% Key definitions for text elements. USE THEM
\def\secref#1{Sec.~\ref{#1}}
\def\figref#1{Fig.~\ref{#1}}
\def\tabref#1{Tab.~\ref{#1}}
\def\eqref#1{Eq.~(\ref{#1})}
\def\algref#1{Alg.~\ref{#1}}
\def\appref#1{App.~\ref{#1}}

\newcommand\etal{\emph{et al.}}

\input{notation.tex}

\title{\LARGE \bf Semantic Mapping Pipeline}

\begin{document}
	
	\maketitle	
	
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=\linewidth]{pics/drawing-crop.pdf}
		\caption{System pipeline.}
		\label{fig:pipeline}
	\end{figure}
	
	\newpage
	\section{Semantic Map}
	
	In our system, a semantic map is a triple:
	
	\begin{equation}
		\cSM = < \cR, \cM, \cP >,
	\end{equation}
	
	\noindent
	with:
	
	\begin{itemize}
		\item $\cR$: global reference system
		\item $\cM$: metric information obtained from sensors
		\item $\cP$: predicates compliant with basic concept hierarchy.
	\end{itemize}
	
	\subsection{Global Reference System}	
	
	The global reference system is typically defined by the mapping module and is used to express all the elements of the map in a single frame that can be used by the robot to fulfill different tasks (e.g. navigation, manipulation, HRI, etc...).
	
	\subsection{Metric Map}	
		
	The metric component $\cM$ of the semantic map can be seen as a collection of 3D objects:
	
	\begin{equation}
		\cM = \{\bO_1 \cdots \bO_N\},
	\end{equation}
	
	\noindent	
	where each object $\bO_i$ is an entity defined by:
	
	\begin{itemize}
		\item {\bf Pose3D}: $\bX = [ \bR | \bt] \in SE(3)$,
		\item {\bf Size3D}: $\bS = <\bL ,\bU> \in \mathbf{R}^3 $,
		\item {\bf Model3D}: $\bM$, e.g. a point cloud, a mesh and so on.
	\end{itemize}
	
	The adopted convention is that the pose $\bX$ is assumed to refer to the centroid of the object and that the bounding box $\bS$ is defined by two 3D vectors that specify its "lower" and "upper" corners.
	
	\subsection{Property Map}	
		
	The semantic component $\cP$ contains the assertions about the properties of the objects in $\cM$:
	
	\begin{equation}
	\cP = \{\bP_1 \cdots \bP_N\},
	\end{equation}
	
	\noindent
	where each $\bP_i$ is a set of predicates referred to the object instance $\bo_i$:
	
	\begin{equation}
		\bP_i = \{ P_1(\bo_i) \cdots P_K(\bo_i) \}
	\end{equation}


	\subsection{Interface}
	
	Possible operations on the map are:
	
	\begin{itemize}
		\item query for object: given an object it's possible to check if it's already present in the map
		\item update object: allows to update an object present in the map with a new observation
		\item add object: allows to add an object to the map if it's seen for the first time
		\item render map: builds a 3d model from the collection of objects, this function can be used for navigation and visualization.
		\item serialize/deserialize: these functions allow to save a map on disk and to load it for later use.
	\end{itemize}
	
	\section{Map Manager}
	
	The map manager has the task of incrementally update the semantic map with new observations coming from sensors. The update of the map can be done by registering a local frame with the global model. Once objects refer to the same global frame, data association is performed to find correspondences between local and global map and update the global map accordingly.
	
	\subsection{Notation}
	
	\begin{itemize}
		\item {\bf Correspondence}: $ \bc^{i,j}_k = <\bO_i^G,\bO_j^L> $
	\end{itemize}
	
	The $k$-th correspondence $ \bc^{i,j}_k$ establishes that the $j$-th object $\bO_j^L$ in the local map is the "closest" (w.r.t. some similarity measure) to the $i$-th object $\bO_i^G$ in the global map.
	
	\subsection{Matching} 
	
	{\bf Problem: }	Finding which objects in the global map $\cM^G$ correspond to the one observed in the local map $\cM^L$. This correspondence is evaluated through a similarity measure $d(O_i^G,O_j^L)$. Thus, finding a correspondence $\bc^{i,j}_k$ consists in finding the object $\bO_j^\star$ that satisfies:
	
	\begin{equation}
		\bO_j^\star = \argmin_{\bO_j} \, d(O_i^G,O_j^L)
	\end{equation}
		
	\noindent
	{\bf Input: }
	\begin{itemize}
		\item {\bf Global Map}: $\cM^G_t = \{ \bO_i^G \}$
		\item {\bf Local Map}: $\cM^L_t = \{ \bO_j^L \}$
	\end{itemize}
	\noindent
	{\bf Output: }
	\begin{itemize}
		\item {\bf Correspondences}: $\cC_t = \{\bc_k\}$		
	\end{itemize}
	\noindent
	{\bf Approaches: }
	\begin{itemize}
		\item brute force
		\item kd-trees
		\item neural networks
	\end{itemize}
	\noindent
	{\bf Remarks: }
	Usually, similarity is computed through euclidean distance. In our problem, it's possible to leverage also semantic information. In a simulated environment, the knowledge of the semantic class is perfect. In real world scenarios, object detectors return a probability distribution over semantic labels. In the first case, the corresponding object is the closest one with the same type. In the second case, euclidean distance could be weighted with class probabilities to represent a form of similarity measure.
		
		
%	\begin{table}
%		\begin{center}
%			\begin{tabular}{|r|l|}
%				\multicolumn{2}{c}{\bf Data Association} \\
%				\hline
%				\multirow{2}{*}{\bf Input:} & local map \\
%				& global map\\ \hline
%				{\bf Output:} & correspondences \\ \hline
%				\multirow{3}{*}{\bf Approaches:} & brute-force \\
%				& kd-trees \\
%				& neural networks\\
%				\hline 
%			\end{tabular}
%		\end{center}
%		\caption{Data association.}
%		\label{tab:assoc}
%	\end{table}
	
	\subsection{Update} 
	
	{\bf Problem: } Integrating incoming information from local map into global map. Steps: for each object in local map check if it's already been seen, yes: merge, no: add to map. 
	
	\noindent
	{\bf Input: }
	\begin{itemize}
		\item {\bf Global Map}: $\cM_t^G = \{ \bO_i^G \}$
		\item {\bf Local Map}: $\cM_t^L = \{ \bO_j^L \}$
		\item {\bf Correspondences}: $\cC_t = \{ \bc_k\}$		
	\end{itemize}
	\noindent
	{\bf Output: }
	\begin{itemize}
		\item {\bf Updated Global Map}: $\cM_{t+1}^G$
	\end{itemize}
	\noindent
	{\bf Approaches: }
	\begin{itemize}
		\item vanilla
		\item age-based
	\end{itemize}
	\noindent
	{\bf Remarks: } This strategy may fail in case of false positives returned by data association. Possible solution:  aging scheme.
	
		
	\section{Action}
	
	\subsection{Exploration} the exploration module has the task of navigating the robot in a way to improve its current knowledge of the environment. 
	
	\subsection{Active Vision} based on the current belief of the problem state, i.e. map and robot pose, and computes a goal pose that maximizes a cost function based on different criteria, e.g., time, space, information gain, human-in-the-loop.
		
	\section{Perception}

	The perception module takes every frame coming from the RGB-D camera and has to tell which objects are seen by the robot. This information will be passed to the map manager so to update the robot knowledge of the environment.
	
	\subsection{Notation}
	
	\begin{itemize}
		\item {\bf Label}: $l_i \in \cL , i=1 \dots N$
		\item {\bf Pixel}: $\bx = (u,v) \subset \mathbf{N}^2 $
%		\item {\bf Size2D}: $\bs = < \btl , \bbr > \in \mathbf{N}^2 $
		\item {\bf RGB image}: $I(\bx) : \bx \rightarrow (r,g,b) \in \mathbf{N}^3$
		\item {\bf Depth image}: $D(\bx) : \bx \rightarrow d \in \mathbf{R}$		
		\item {\bf Label image}: $L(\bx) : \bx \rightarrow l_i \in \cL$		
%		\item {\bf Detection}: $ \bd = < l, \bx, \bs > $
	\end{itemize}
	
	\subsection{Filtering}
	
	{\bf Problem: } Decide which frame to consider.

	\noindent
	{\bf Input: }
	\begin{itemize}
		\item {\bf RGB Image}: $I$
		\item {\bf Depth Image}: $D$
	\end{itemize}
	\noindent
	{\bf Output: }
	\begin{itemize}
		\item {\bf Trigger}: $t = \{0,1\}$
	\end{itemize}
	\noindent
	{\bf Approaches: }
	\begin{itemize}
		\item \dots
	\end{itemize}
	\noindent
	{\bf Remarks: } \dots
	
	\subsection{Recognition}
	 
	{\bf Problem: } Finding instances of known class objects in the input image. More formally, given an RGB image $I$ and a set of semantic labels $\cL$, computes an assignment $L$ of labels $l_i$ to each image pixel $\bx$.
	
	\noindent
	{\bf Input: }
	\begin{itemize}
		\item {\bf RGB Image}: $I$
		\item {\bf Semantic Labels}: $\cL = \{l_i\}$
	\end{itemize}
	\noindent
	{\bf Output: }
	\begin{itemize}
		\item {\bf Label Image}: $L$
	\end{itemize}
	\noindent
	{\bf Approaches: }
	\begin{itemize}
		\item appearance (all computer vision approaches)
		\item geometry (3d object recognition)
	\end{itemize}
	\noindent
	{\bf Remarks: } In a simulated environment it's possible to execute this task by performing only geometrical computations, since one knows the pose of the objects and the robot in the world \dots
	
	\subsection{Extraction}
	
	{\bf Problem: } Building local map from the output of recognition. The first step is to group together pixels with the same semantic label $l_i$. Next, with the depth image $D$, it's possible to recover the 3D position of each image pixel. From this information, the object pose and its size can be inferred, so to build an object:
	
	\begin{equation}
	 \bO = < \bT, \bX, \bS > 
	\end{equation}
	
	\noindent
	{\bf Input: }
	\begin{itemize}
		\item {\bf Label Image}: $L$
		\item {\bf Depth Image}: $D$
	\end{itemize}
	\noindent
	{\bf Output: }
	\begin{itemize}
		\item {\bf Local Map}: $\cM^L = \{ \bO_j^L \}$
	\end{itemize}
	\noindent
	{\bf Approaches: }
	\begin{itemize}
		\item depend on the recognition module
	\end{itemize}
	\noindent
	{\bf Remarks: } \dots
	
\end{document}
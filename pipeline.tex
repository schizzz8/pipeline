\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}

%% Key definitions for text elements. USE THEM
\def\secref#1{Sec.~\ref{#1}}
\def\figref#1{Fig.~\ref{#1}}
\def\tabref#1{Tab.~\ref{#1}}
\def\eqref#1{Eq.~(\ref{#1})}
\def\algref#1{Alg.~\ref{#1}}
\def\appref#1{App.~\ref{#1}}

\newcommand\etal{\emph{et al.}}

\input{notation.tex}

\title{\LARGE \bf Semantic Mapping Pipeline}

\begin{document}
	
	\maketitle	
	
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=\linewidth]{pics/drawing-crop.pdf}
		\caption{System pipeline.}
		\label{fig:pipeline}
	\end{figure}
	
	\section{Semantic Map}
	
	In our system, a semantic map is a triple:
	
	\begin{equation}
		\cSM = < \cR, \cM, \cP >,
	\end{equation}
	
	\noindent
	with:
	
	\begin{itemize}
		\item $\cR$: global reference system
		\item $\cM$: metric information obtained from sensors
		\item $\cP$: predicates compliant with basic concept hierarchy
	\end{itemize}
	
	More in detail, $\cM = \{\bO_i\}_{i=1 \dots N}$ is a collection of 3D objects, where each object is defined by:
	
	\begin{itemize}
		\item {\bf Label}: $l_i \in \mathcal{L}$,
		\item {\bf Pose3D}: $\bX = [ \bR | \bt] \in SE(3)$,
		\item {\bf Size3D}: $\bB = (\bm ,\bM) \in \mathbf{R}^{3 \times 2}$,
	\end{itemize}
	
	\noindent
	and has a 3D model attached to it, e.g. point cloud, mesh and so on. Possible operations on the map are:
	
	\begin{itemize}
		\item query for object: given an object it's possible to check if it's already present in the map
		\item update object: allows to update an object present in the map with a new observation
		\item add object: allows to add an object to the map if it's seen for the first time
		\item render map: builds a 3d model from the collection of objects, this function can be used for navigation and visualization.
		\item serialize/deserialize: these functions allow to save a map on disk and to load it for later use.
	\end{itemize}
	
	\section{Map Manager}
	
	The map manager has the task of incrementally updating the semantic map with new observations coming from sensors. The update of the map can be done by registering a local frame with the global model. Once objects are transformed in global frame, data association is performed to find correspondences between local and global map (object tracking) and update the global map accordingly.
	
	\subsection{Data association} 
	
	Finds which objects in the global map $\cM^G$ correspond to the one observed in the local map $\cM^L$ (if any). This correspondence is evaluated through a similarity measure $d(O_i^G,O_j^L)$. So that, finding a correspondence $\bc_{i,j} = (\bO_i^G,\bO_j^L)$ consists in finding the object $\bO_j^L$ that satisfies:
	
	\begin{equation}
		\bO_j^L = \argmin_{\bO_j^L}  d(O_i^G,O_j^L)
	\end{equation}
	
	Usually, similarity is computed through euclidean distance. In our problem, it's possible to leverage also semantic information. In a simulated environment, the knowledge of the semantic class is perfect. In real world scenarios, object detectors return a probability distribution over semantic labels. In the first case, the corresponding object is the closest one with the same type. In the second case, euclidean distance could be weighted with class probabilities to represent a form of similarity measure.
	
	\hspace{1cm}
	
	\noindent
	{\bf Input: }
	\begin{itemize}
		\item {\bf Global Map}: $\cM^G = \{ \bO_i^G \}_{i=1 \dots N}$
		\item {\bf Local Map}: $\cM^L = \{ \bO_j^L \}_{j=1 \dots M}$
	\end{itemize}
	\noindent
	{\bf Output: }
	\begin{itemize}
		\item {\bf Correspondences}: $\cC = \{ \bc_{i,j}\}$		
	\end{itemize}
	\noindent
	{\bf Approaches: }
	\begin{itemize}
		\item brute force
		\item kd-trees
		\item neural networks
	\end{itemize}
		
%	\begin{table}
%		\begin{center}
%			\begin{tabular}{|r|l|}
%				\multicolumn{2}{c}{\bf Data Association} \\
%				\hline
%				\multirow{2}{*}{\bf Input:} & local map \\
%				& global map\\ \hline
%				{\bf Output:} & correspondences \\ \hline
%				\multirow{3}{*}{\bf Approaches:} & brute-force \\
%				& kd-trees \\
%				& neural networks\\
%				\hline 
%			\end{tabular}
%		\end{center}
%		\caption{Data association.}
%		\label{tab:assoc}
%	\end{table}
	
	\subsection{Data integration} 
	
	Integrates incoming information from local map into global map. Input: maps, correspondences. Output: updated global map. Steps: for each object in local map check if it's already been seen, yes: merge, no: add to map. This strategy may fail in case of false positives returned by data association. Possible solution:  aging scheme.
	
	
	\hspace{1cm}
	
	\noindent
	{\bf Input: }
	\begin{itemize}
		\item {\bf Global Map}: $\cM_t^G = \{ \bO_i^G \}_{i=1 \dots N}$
		\item {\bf Local Map}: $\cM_t^L = \{ \bO_j^L \}_{j=1 \dots M}$
		\item {\bf Correspondences}: $\cC = \{ \bc_{i,j}\}$		
	\end{itemize}
	\noindent
	{\bf Output: }
	\begin{itemize}
		\item {\bf Updated Global Map}: $\cM_{t+1}^G$
	\end{itemize}
	\noindent
	{\bf Approaches: }
	\begin{itemize}
		\item vanilla
		\item age-based
	\end{itemize}
		
	\section{Action Manager}
	
	\subsection{Exploration} the exploration module has the task of navigating the robot in a way to improve its current knowledge of the environment. 
	
	\subsection{Active Vision} based on the current belief of the problem state, i.e. map and robot pose, and computes a goal pose that maximizes a cost function based on different criteria, e.g., time, space, information gain, human-in-the-loop.
		
	\section{Perception}

	Our processing pipeline is made of three modules that exchange data structures among each other to build and maintain the semantic map.
	
	\subsection{Filtering}
	
	This module is in charge of triggering the acquisition of sensor data to update the semantic map.

	\subsection{Detection \& Classification}
	
	The perception module is in charge of localizing and classifying objects in the current frame to label each image pixel with its semantic class. More formally, given an image $I \subset \mathbf{N}^{3 \times 2}$ and a set of semantic labels $\mathcal{L} = \{l_i\}_{i=1 \dots L}$, computes an assignment of labels $l_i \in \mathcal{L}$ to each image pixel $x = (u,v) \in I$ .
	
	\subsection{Localization \& Extraction}
	
	Feature extraction is performed by taking the output of the perception module and building a 3D object (atomic entity of the map) from each detection. After that, by knowing the pose of the robot in the global frame, it's possible to transform the extracted objects in the global frame to find correspondences (data association).
	
\end{document}